# transformer_from_scratch

This repository provides a step-by-step implementation of a language model using the Transformer architecture, using the PyTorch library. The goal is to build and understand the inner workings of the Transformer model by building it incrementally, starting from a simple benchmark and gradually adding more complex components.

## Table of Contents

- [Description](#description)
- [Installation](#installation)
- [Usage](#usage)
- [Examples](#examples)
- [Contributing](#contributing)
- [License](#license)

## Description

The `transformer_from_scratch` repository aims to demystify the Transformer architecture by breaking it down into smaller components and providing a clear, step-by-step implementation. By going through the code, you get a deeper understanding of how the self-attention mechanism and multi-head attention work, and how they come together to form the Transformer model.

The implementation is kept simple and easy to follow, using the PyTorch library, making it accessible for both beginners and those with some experience in deep learning.

## Installation

To use the code in this repository, you need to have Python 3 and PyTorch installed. You can install the necessary dependencies by running the following command:
```shell
pip install -r requirements.txt
```

## Usage

Once you have installed the dependencies, you can run the different components of the Transformer model individually. Each component is implemented as a separate Python module and can be executed independently. You can compare the performance of the different blocks and see how it increases as more complexity is added.

## Examples

## License
This repository is licensed under the MIT License. You are free to use, modify, and distribute the code in this repository as per the terms of the license.
