# transformer_from_scratch

This repository provides a step-by-step implementation of a language model using the Transformer architecture, using the PyTorch library. The goal is to build and understand the inner workings of the Transformer model by building it incrementally, starting from a simple benchmark and gradually adding more complex components.

## Table of Contents

- [Description](#description)
- [Installation](#installation)
- [Usage](#usage)
- [Examples](#examples)
- [Contributing](#contributing)
- [License](#license)

## Description

The `transformer_from_scratch` repository aims to demystify the Transformer architecture by breaking it down into smaller components and providing a clear, step-by-step implementation. By going through the code, you get a deeper understanding of how the self-attention mechanism and multi-head attention work, and how they come together to form the Transformer model.

The implementation is kept simple and easy to follow, using the PyTorch library, making it accessible for both beginners and those with some experience in deep learning.

## Installation

To use the code in this repository, you need to have Python 3 and PyTorch installed. You can install the necessary dependencies by running the following command:
